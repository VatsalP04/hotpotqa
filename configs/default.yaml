# Default configuration for HotpotQA NLP Pipeline
# This file contains all default settings for data loading, preprocessing, and model training

project:
  name: "hotpotqa-nlp-pipeline"
  version: "1.0.0"
  random_seed: 42

# Data paths and settings
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  cache_dir: "data/cache"

  # HotpotQA dataset files
  train_file: "hotpot_train_v1.1.json"
  dev_file: "hotpot_dev_distractor_v1.json"

  # Embeddings (pretrained vectors)
  # Prefer keeping large embedding files under data/embeddings and configure the
  # exact path below. You can override these in a custom config.
  embeddings_dir: "data/embeddings"
  embeddings_file: "glove.840B.300d.txt"

  # Data loading options
  max_examples: null  # null means load all examples
  shuffle_seed: 42

# Preprocessing settings
preprocessing:
  # Text cleaning
  cleaning:
    lowercase: false  # Preserve case for NER and other tasks
    remove_extra_whitespace: true
    normalize_unicode: true
    remove_html: true

  # Tokenization
  tokenization:
    tokenizer_type: "huggingface"  # Options: "huggingface", "spacy", "nltk"
    model_name: "bert-base-uncased"  # For HuggingFace tokenizers
    max_length: 512
    truncation: true
    padding: "max_length"
    add_special_tokens: true
    return_tensors: null  # null, "pt", "tf", "np"

  # Context processing
  context:
    max_context_length: 2048  # Maximum total context tokens
    max_paragraphs: 10  # Maximum number of paragraphs to include
    paragraph_separator: "\n\n"
    include_titles: true  # Include article titles in context
    title_template: "Title: {title}\n"

  # Multi-hop specific preprocessing
  multihop:
    extract_supporting_facts: true
    create_chains: true  # Create reasoning chains from supporting facts
    max_hops: 4

  # Output format
  output:
    save_format: "json"  # Options: "json", "jsonl", "pickle"
    include_raw: false  # Include raw examples in processed data
    include_metadata: true

# Logging settings
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_dir: "logs"
  log_file: "pipeline.log"
  console_output: true

# Model settings (for future use)
model:
  name: "mistral-large-latest"
  temperature: 0.2
  max_tokens: 512

# Evaluation settings
evaluation:
  metrics: ["em", "f1", "sp_em", "sp_f1", "joint_em", "joint_f1"]
  save_predictions: true
  prediction_dir: "data/processed/predictions"
