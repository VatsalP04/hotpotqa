{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d63f3cf",
   "metadata": {},
   "source": [
    "# HotpotQA Dataset - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of the HotpotQA dataset including:\n",
    "- Dataset statistics\n",
    "- Question and answer analysis\n",
    "- Context and supporting facts analysis\n",
    "- Visualizations\n",
    "- Train vs Dev comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb694ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a48029",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define utility functions for loading data and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hotpot(split: str = \"train\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Loads the HotpotQA dataset.\n",
    "    Args:\n",
    "        split (str): 'train' or 'dev' to choose which split to load.\n",
    "    Returns:\n",
    "        data (list): List of QA examples (each a dictionary).\n",
    "    \"\"\"\n",
    "    base_path = \"data/hotpotqa\"\n",
    "    file_map = {\n",
    "        \"train\": \"hotpot_train_v1.1.json\",\n",
    "        \"dev\": \"hotpot_dev_distractor_v1.json\"\n",
    "    }\n",
    "\n",
    "    file_path = os.path.join(base_path, file_map[split])\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at {file_path}\")\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"✅ Loaded {len(data):,} {split} examples from {file_path}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e3984",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and development datasets\n",
    "train_data = load_hotpot(\"train\")\n",
    "dev_data = load_hotpot(\"dev\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb9862",
   "metadata": {},
   "source": [
    "## 1. Basic Statistics\n",
    "\n",
    "Compute basic statistics about the dataset including question/answer lengths, context statistics, and supporting facts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(data: List[Dict], split_name: str = \"dataset\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute basic statistics about the dataset.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASIC STATISTICS - {split_name.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    stats = {\n",
    "        'Total Examples': len(data),\n",
    "        'Unique Questions': len(set(ex['question'] for ex in data)),\n",
    "        'Unique Answers': len(set(ex['answer'] for ex in data)),\n",
    "    }\n",
    "    \n",
    "    # Question statistics\n",
    "    question_lengths = [len(ex['question'].split()) for ex in data]\n",
    "    stats['Avg Question Length (words)'] = np.mean(question_lengths)\n",
    "    stats['Median Question Length (words)'] = np.median(question_lengths)\n",
    "    stats['Min Question Length (words)'] = np.min(question_lengths)\n",
    "    stats['Max Question Length (words)'] = np.max(question_lengths)\n",
    "    \n",
    "    # Answer statistics\n",
    "    answer_lengths = [len(ex['answer'].split()) for ex in data]\n",
    "    stats['Avg Answer Length (words)'] = np.mean(answer_lengths)\n",
    "    stats['Median Answer Length (words)'] = np.median(answer_lengths)\n",
    "    stats['Min Answer Length (words)'] = np.min(answer_lengths)\n",
    "    stats['Max Answer Length (words)'] = np.max(answer_lengths)\n",
    "    \n",
    "    # Context statistics\n",
    "    num_contexts = [len(ex['context']) for ex in data]\n",
    "    stats['Avg Number of Context Articles'] = np.mean(num_contexts)\n",
    "    stats['Median Number of Context Articles'] = np.median(num_contexts)\n",
    "    stats['Min Number of Context Articles'] = np.min(num_contexts)\n",
    "    stats['Max Number of Context Articles'] = np.max(num_contexts)\n",
    "    \n",
    "    # Supporting facts statistics\n",
    "    num_supporting_facts = [len(ex.get('supporting_facts', [])) for ex in data]\n",
    "    stats['Avg Number of Supporting Facts'] = np.mean(num_supporting_facts)\n",
    "    stats['Median Number of Supporting Facts'] = np.median(num_supporting_facts)\n",
    "    stats['Min Number of Supporting Facts'] = np.min(num_supporting_facts)\n",
    "    stats['Max Number of Supporting Facts'] = np.max(num_supporting_facts)\n",
    "    \n",
    "    # Total sentences in context\n",
    "    total_sentences = []\n",
    "    for ex in data:\n",
    "        total = sum(len(article[1]) for article in ex['context'])\n",
    "        total_sentences.append(total)\n",
    "    stats['Avg Total Sentences per Example'] = np.mean(total_sentences)\n",
    "    stats['Median Total Sentences per Example'] = np.median(total_sentences)\n",
    "    \n",
    "    df_stats = pd.DataFrame([stats]).T\n",
    "    df_stats.columns = ['Value']\n",
    "    print(df_stats.to_string())\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "# Analyze train split\n",
    "train_stats = basic_statistics(train_data, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dev split\n",
    "dev_stats = basic_statistics(dev_data, \"dev\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd80386",
   "metadata": {},
   "source": [
    "## 2. Question Type Analysis\n",
    "\n",
    "Analyze the distribution of question types based on question words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98904bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question_types(data: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze question types based on question words.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUESTION TYPE ANALYSIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    question_words = ['what', 'who', 'where', 'when', 'why', 'how', 'which', 'is', 'are', 'was', 'were']\n",
    "    question_type_counts = Counter()\n",
    "    \n",
    "    for ex in data:\n",
    "        question_lower = ex['question'].lower()\n",
    "        found = False\n",
    "        for qw in question_words:\n",
    "            if question_lower.startswith(qw):\n",
    "                question_type_counts[qw] += 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            question_type_counts['other'] += 1\n",
    "    \n",
    "    df_qtypes = pd.DataFrame(list(question_type_counts.items()), \n",
    "                            columns=['Question Type', 'Count'])\n",
    "    df_qtypes['Percentage'] = (df_qtypes['Count'] / len(data) * 100).round(2)\n",
    "    df_qtypes = df_qtypes.sort_values('Count', ascending=False)\n",
    "    \n",
    "    print(df_qtypes.to_string(index=False))\n",
    "    \n",
    "    return question_type_counts\n",
    "\n",
    "# Analyze train split\n",
    "train_qtypes = analyze_question_types(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d658901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dev split\n",
    "dev_qtypes = analyze_question_types(dev_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be0483",
   "metadata": {},
   "source": [
    "## 3. Answer Type Analysis\n",
    "\n",
    "Analyze answer types (yes/no, entity, number, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174be8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_answer_types(data: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze answer types (yes/no, entity, number, etc.)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ANSWER TYPE ANALYSIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    answer_types = Counter()\n",
    "    \n",
    "    for ex in data:\n",
    "        answer = ex['answer'].lower().strip()\n",
    "        if answer in ['yes', 'no']:\n",
    "            answer_types['Yes/No'] += 1\n",
    "        elif answer.isdigit():\n",
    "            answer_types['Number'] += 1\n",
    "        elif len(answer.split()) == 1:\n",
    "            answer_types['Single Word'] += 1\n",
    "        elif len(answer.split()) <= 3:\n",
    "            answer_types['Short Phrase (2-3 words)'] += 1\n",
    "        else:\n",
    "            answer_types['Long Answer'] += 1\n",
    "    \n",
    "    df_atypes = pd.DataFrame(list(answer_types.items()), \n",
    "                            columns=['Answer Type', 'Count'])\n",
    "    df_atypes['Percentage'] = (df_atypes['Count'] / len(data) * 100).round(2)\n",
    "    df_atypes = df_atypes.sort_values('Count', ascending=False)\n",
    "    \n",
    "    print(df_atypes.to_string(index=False))\n",
    "    \n",
    "    return answer_types\n",
    "\n",
    "# Analyze train split\n",
    "train_atypes = analyze_answer_types(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dev split\n",
    "dev_atypes = analyze_answer_types(dev_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fa480",
   "metadata": {},
   "source": [
    "## 4. Supporting Facts Analysis\n",
    "\n",
    "Analyze supporting facts distribution and patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b53b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_supporting_facts(data: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze supporting facts distribution and patterns.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUPPORTING FACTS ANALYSIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    num_supporting_facts = [len(ex.get('supporting_facts', [])) for ex in data]\n",
    "    \n",
    "    stats = {\n",
    "        'Mean': np.mean(num_supporting_facts),\n",
    "        'Median': np.median(num_supporting_facts),\n",
    "        'Std': np.std(num_supporting_facts),\n",
    "        'Min': np.min(num_supporting_facts),\n",
    "        'Max': np.max(num_supporting_facts),\n",
    "    }\n",
    "    \n",
    "    # Count distribution\n",
    "    fact_count_dist = Counter(num_supporting_facts)\n",
    "    df_dist = pd.DataFrame(list(fact_count_dist.items()), \n",
    "                          columns=['Number of Supporting Facts', 'Count'])\n",
    "    df_dist = df_dist.sort_values('Number of Supporting Facts')\n",
    "    df_dist['Percentage'] = (df_dist['Count'] / len(data) * 100).round(2)\n",
    "    \n",
    "    print(\"Supporting Facts Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\nDistribution:\")\n",
    "    print(df_dist.to_string(index=False))\n",
    "    \n",
    "    # Analyze which articles contain supporting facts\n",
    "    articles_with_supporting = []\n",
    "    for ex in data:\n",
    "        supporting_facts = ex.get('supporting_facts', [])\n",
    "        article_titles = [fact[0] for fact in supporting_facts]\n",
    "        articles_with_supporting.append(len(set(article_titles)))\n",
    "    \n",
    "    print(f\"\\nArticles with Supporting Facts:\")\n",
    "    print(f\"  Mean: {np.mean(articles_with_supporting):.2f}\")\n",
    "    print(f\"  Median: {np.median(articles_with_supporting):.2f}\")\n",
    "    print(f\"  Max: {np.max(articles_with_supporting)}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze train split\n",
    "train_sf_stats = analyze_supporting_facts(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884172b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dev split\n",
    "dev_sf_stats = analyze_supporting_facts(dev_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd2865",
   "metadata": {},
   "source": [
    "## 5. Context Analysis\n",
    "\n",
    "Analyze context articles and sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_context(data: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze context articles and sentences.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CONTEXT ANALYSIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    num_articles = []\n",
    "    num_sentences = []\n",
    "    article_lengths = []\n",
    "    \n",
    "    for ex in data:\n",
    "        context = ex['context']\n",
    "        num_articles.append(len(context))\n",
    "        \n",
    "        total_sentences = 0\n",
    "        for title, sentences in context:\n",
    "            num_sents = len(sentences)\n",
    "            total_sentences += num_sents\n",
    "            article_lengths.append(num_sents)\n",
    "        \n",
    "        num_sentences.append(total_sentences)\n",
    "    \n",
    "    stats = {\n",
    "        'Articles per Example': {\n",
    "            'Mean': np.mean(num_articles),\n",
    "            'Median': np.median(num_articles),\n",
    "            'Min': np.min(num_articles),\n",
    "            'Max': np.max(num_articles),\n",
    "        },\n",
    "        'Sentences per Example': {\n",
    "            'Mean': np.mean(num_sentences),\n",
    "            'Median': np.median(num_sentences),\n",
    "            'Min': np.min(num_sentences),\n",
    "            'Max': np.max(num_sentences),\n",
    "        },\n",
    "        'Sentences per Article': {\n",
    "            'Mean': np.mean(article_lengths),\n",
    "            'Median': np.median(article_lengths),\n",
    "            'Min': np.min(article_lengths),\n",
    "            'Max': np.max(article_lengths),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, values in stats.items():\n",
    "        print(f\"{category}:\")\n",
    "        for key, value in values.items():\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze train split\n",
    "train_context_stats = analyze_context(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef697c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dev split\n",
    "dev_context_stats = analyze_context(dev_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb137e",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "Create comprehensive visualizations of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(data: List[Dict], split_name: str = \"dataset\", save_dir: str = \"eda_plots\", show_plots: bool = True):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CREATING VISUALIZATIONS - {split_name.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    question_lengths = [len(ex['question'].split()) for ex in data]\n",
    "    answer_lengths = [len(ex['answer'].split()) for ex in data]\n",
    "    num_contexts = [len(ex['context']) for ex in data]\n",
    "    num_supporting_facts = [len(ex.get('supporting_facts', [])) for ex in data]\n",
    "    total_sentences = [sum(len(article[1]) for article in ex['context']) for ex in data]\n",
    "    \n",
    "    # 1. Question Length Distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(question_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Question Length (words)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Question Lengths')\n",
    "    plt.axvline(np.mean(question_lengths), color='r', linestyle='--', \n",
    "                label=f'Mean: {np.mean(question_lengths):.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(question_lengths, vert=True)\n",
    "    plt.ylabel('Question Length (words)')\n",
    "    plt.title('Box Plot of Question Lengths')\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/question_lengths_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: question_lengths_{split_name}.png\")\n",
    "    \n",
    "    # 2. Answer Length Distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(answer_lengths, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "    plt.xlabel('Answer Length (words)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Answer Lengths')\n",
    "    plt.axvline(np.mean(answer_lengths), color='r', linestyle='--', \n",
    "                label=f'Mean: {np.mean(answer_lengths):.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(answer_lengths, vert=True)\n",
    "    plt.ylabel('Answer Length (words)')\n",
    "    plt.title('Box Plot of Answer Lengths')\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/answer_lengths_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: answer_lengths_{split_name}.png\")\n",
    "    \n",
    "    # 3. Context Articles Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    context_counts = Counter(num_contexts)\n",
    "    plt.bar(context_counts.keys(), context_counts.values(), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Number of Context Articles')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Number of Context Articles per Example')\n",
    "    plt.xticks(list(context_counts.keys()))\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/context_articles_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: context_articles_{split_name}.png\")\n",
    "    \n",
    "    # 4. Supporting Facts Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    fact_counts = Counter(num_supporting_facts)\n",
    "    plt.bar(fact_counts.keys(), fact_counts.values(), edgecolor='black', alpha=0.7, color='green')\n",
    "    plt.xlabel('Number of Supporting Facts')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Number of Supporting Facts per Example')\n",
    "    plt.xticks(list(fact_counts.keys()))\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/supporting_facts_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: supporting_facts_{split_name}.png\")\n",
    "    \n",
    "    # 5. Question Type Distribution\n",
    "    question_words = ['what', 'who', 'where', 'when', 'why', 'how', 'which', 'is', 'are', 'was', 'were']\n",
    "    question_type_counts = Counter()\n",
    "    for ex in data:\n",
    "        question_lower = ex['question'].lower()\n",
    "        found = False\n",
    "        for qw in question_words:\n",
    "            if question_lower.startswith(qw):\n",
    "                question_type_counts[qw] += 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            question_type_counts['other'] += 1\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    qtypes = list(question_type_counts.keys())\n",
    "    counts = list(question_type_counts.values())\n",
    "    plt.barh(qtypes, counts, edgecolor='black', alpha=0.7, color='purple')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Question Type')\n",
    "    plt.title('Distribution of Question Types')\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/question_types_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: question_types_{split_name}.png\")\n",
    "    \n",
    "    # 6. Scatter Plot: Question Length vs Answer Length\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(question_lengths, answer_lengths, alpha=0.5, s=20)\n",
    "    plt.xlabel('Question Length (words)')\n",
    "    plt.ylabel('Answer Length (words)')\n",
    "    plt.title('Question Length vs Answer Length')\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/question_vs_answer_length_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: question_vs_answer_length_{split_name}.png\")\n",
    "    \n",
    "    # 7. Total Sentences Distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(total_sentences, bins=30, edgecolor='black', alpha=0.7, color='teal')\n",
    "    plt.xlabel('Total Sentences per Example')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Total Sentences in Context')\n",
    "    plt.axvline(np.mean(total_sentences), color='r', linestyle='--', \n",
    "                label=f'Mean: {np.mean(total_sentences):.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(total_sentences, vert=True)\n",
    "    plt.ylabel('Total Sentences per Example')\n",
    "    plt.title('Box Plot of Total Sentences')\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/total_sentences_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: total_sentences_{split_name}.png\")\n",
    "    \n",
    "    # 8. Correlation Heatmap\n",
    "    df_corr = pd.DataFrame({\n",
    "        'Question Length': question_lengths,\n",
    "        'Answer Length': answer_lengths,\n",
    "        'Num Contexts': num_contexts,\n",
    "        'Num Supporting Facts': num_supporting_facts,\n",
    "        'Total Sentences': total_sentences\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of Dataset Features')\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}/correlation_heatmap_{split_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    print(f\"✅ Saved: correlation_heatmap_{split_name}.png\")\n",
    "    \n",
    "    print(f\"\\n✅ All visualizations saved to '{save_dir}/' directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b1406",
   "metadata": {},
   "source": [
    "### Train Split Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d50514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for train split\n",
    "create_visualizations(train_data, \"train\", show_plots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61089aa8",
   "metadata": {},
   "source": [
    "### Dev Split Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for dev split\n",
    "create_visualizations(dev_data, \"dev\", show_plots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c8299",
   "metadata": {},
   "source": [
    "## 7. Train vs Dev Comparison\n",
    "\n",
    "Compare train and dev splits side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd761bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_splits(train_data: List[Dict], dev_data: List[Dict]):\n",
    "    \"\"\"\n",
    "    Compare train and dev splits.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAIN vs DEV COMPARISON\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    comparisons = {\n",
    "        'Total Examples': [len(train_data), len(dev_data)],\n",
    "        'Avg Question Length': [\n",
    "            np.mean([len(ex['question'].split()) for ex in train_data]),\n",
    "            np.mean([len(ex['question'].split()) for ex in dev_data])\n",
    "        ],\n",
    "        'Avg Answer Length': [\n",
    "            np.mean([len(ex['answer'].split()) for ex in train_data]),\n",
    "            np.mean([len(ex['answer'].split()) for ex in dev_data])\n",
    "        ],\n",
    "        'Avg Context Articles': [\n",
    "            np.mean([len(ex['context']) for ex in train_data]),\n",
    "            np.mean([len(ex['context']) for ex in dev_data])\n",
    "        ],\n",
    "        'Avg Supporting Facts': [\n",
    "            np.mean([len(ex.get('supporting_facts', [])) for ex in train_data]),\n",
    "            np.mean([len(ex.get('supporting_facts', [])) for ex in dev_data])\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    df_compare = pd.DataFrame(comparisons, index=['Train', 'Dev'])\n",
    "    print(df_compare.T.to_string())\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    os.makedirs(\"eda_plots\", exist_ok=True)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics = list(comparisons.keys())\n",
    "    train_values = [comparisons[m][0] for m in metrics]\n",
    "    dev_values = [comparisons[m][1] for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        bars1 = ax.bar(x[i] - width/2, train_values[i], width, label='Train', alpha=0.8)\n",
    "        bars2 = ax.bar(x[i] + width/2, dev_values[i], width, label='Dev', alpha=0.8)\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title(metric)\n",
    "        ax.legend()\n",
    "        ax.set_xticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_plots/train_dev_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n✅ Saved: train_dev_comparison.png\")\n",
    "    \n",
    "    return df_compare\n",
    "\n",
    "# Compare splits\n",
    "comparison_df = compare_splits(train_data, dev_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1b8f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This EDA provides comprehensive insights into the HotpotQA dataset:\n",
    "\n",
    "- **Dataset Statistics**: Basic metrics about questions, answers, context, and supporting facts\n",
    "- **Question Types**: Distribution of different question types\n",
    "- **Answer Types**: Patterns in answer formats\n",
    "- **Supporting Facts**: Analysis of multi-hop reasoning requirements\n",
    "- **Context Analysis**: Understanding of document complexity\n",
    "- **Visualizations**: Comprehensive plots for all metrics\n",
    "- **Split Comparison**: Train vs Dev differences\n",
    "\n",
    "All visualizations are saved to the `eda_plots/` directory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
