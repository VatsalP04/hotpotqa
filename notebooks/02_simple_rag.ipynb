{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG System for HotpotQA\n",
    "\n",
    "Now that you understand the data, let's build a **basic RAG (Retrieval-Augmented Generation) system**.\n",
    "\n",
    "## Pipeline:\n",
    "1. **Retrieve** relevant paragraphs from the 10 available\n",
    "2. **Generate** answer using Mistral with retrieved context\n",
    "3. **Evaluate** on dev set\n",
    "\n",
    "We'll start with simple retrieval and improve it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/vatsalpatel/hotpotqa\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Setup\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 7,405 dev examples\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "def load_data(split='dev'):\n",
    "    if split == 'train':\n",
    "        file_path = project_root / 'data/raw/hotpot_train_v1.1.json'\n",
    "    else:\n",
    "        file_path = project_root / 'data/raw/hotpot_dev_distractor_v1.json'\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(data):,} {split} examples\")\n",
    "    return data\n",
    "\n",
    "dev_data = load_data('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "\n",
      "Retrieved paragraphs:\n",
      "\n",
      "1. Title: Doctor Strange (2016 film)\n",
      "Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Stud...\n",
      "\n",
      "2. Title: Tyler Bates\n",
      "Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.  Much of his work is in the action and horror film genr...\n",
      "\n",
      "3. Title: Deliver Us from Evil (2014 film)\n",
      "Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.  The film is officially based o...\n"
     ]
    }
   ],
   "source": [
    "def simple_keyword_retrieval(question, context_paragraphs, top_k=3):\n",
    "    \"\"\"\n",
    "    Simple retrieval: score paragraphs by keyword overlap with question\n",
    "    \n",
    "    Args:\n",
    "        question: Question string\n",
    "        context_paragraphs: List of [title, sentences] pairs\n",
    "        top_k: Number of paragraphs to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of top_k paragraphs as formatted strings\n",
    "    \"\"\"\n",
    "    question_words = set(question.lower().split())\n",
    "    \n",
    "    # Score each paragraph\n",
    "    scored = []\n",
    "    for title, sentences in context_paragraphs:\n",
    "        # Combine title and sentences\n",
    "        text = title + ' ' + ' '.join(sentences)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Count question words in paragraph\n",
    "        score = sum(1 for word in question_words if word in text_lower)\n",
    "        \n",
    "        scored.append((score, title, sentences, text))\n",
    "    \n",
    "    # Sort by score (highest first) and take top_k\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # Format retrieved paragraphs\n",
    "    retrieved = []\n",
    "    for score, title, sentences, text in scored[:top_k]:\n",
    "        retrieved.append(f\"Title: {title}\\n{' '.join(sentences)}\")\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "# Test on one example\n",
    "test_ex = dev_data[0]\n",
    "retrieved = simple_keyword_retrieval(test_ex['question'], test_ex['context'], top_k=3)\n",
    "\n",
    "print(\"Question:\", test_ex['question'])\n",
    "print(\"\\nRetrieved paragraphs:\")\n",
    "for i, para in enumerate(retrieved, 1):\n",
    "    print(f\"\\n{i}. {para[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Complete RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG system initialized\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Simple RAG system for HotpotQA\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None, model=\"mistral-large-latest\"):\n",
    "        self.client = Mistral(api_key=api_key or os.getenv('MISTRAL_API_KEY'))\n",
    "        self.model = model\n",
    "    \n",
    "    def retrieve(self, question, context_paragraphs, top_k=3):\n",
    "        \"\"\"Retrieve top_k relevant paragraphs\"\"\"\n",
    "        return simple_keyword_retrieval(question, context_paragraphs, top_k)\n",
    "    \n",
    "    def generate(self, question, context_paragraphs):\n",
    "        \"\"\"Generate answer using Mistral\"\"\"\n",
    "        context_text = \"\\n\\n\".join(context_paragraphs)\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the provided context. Be concise and direct.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    def answer(self, example, top_k=3, verbose=False):\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline for one example\n",
    "        \n",
    "        Returns:\n",
    "            dict with 'answer' and 'retrieved_paragraphs'\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve\n",
    "        retrieved = self.retrieve(example['question'], example['context'], top_k)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Retrieved {len(retrieved)} paragraphs\")\n",
    "        \n",
    "        # Step 2: Generate\n",
    "        answer = self.generate(example['question'], retrieved)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'retrieved_paragraphs': retrieved\n",
    "        }\n",
    "\n",
    "# Initialize RAG\n",
    "rag = SimpleRAG()\n",
    "print(\"‚úÖ RAG system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Few Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 3 examples\n",
    "import random\n",
    "\n",
    "test_examples = random.sample(dev_data, 3)\n",
    "\n",
    "for i, ex in enumerate(test_examples, 1):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Example {i}/3\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = rag.answer(ex, top_k=3, verbose=True)\n",
    "    \n",
    "    print(f\"\\nQuestion: {ex['question']}\")\n",
    "    print(f\"\\nü§ñ Predicted: {result['answer']}\")\n",
    "    print(f\"‚úÖ Ground Truth: {ex['answer']}\")\n",
    "    print(f\"\\nüìä Match: {'YES ‚úì' if result['answer'].lower() == ex['answer'].lower() else 'NO ‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Larger Sample\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This will use API calls. Start with 10-20 examples to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_rag(rag, examples, top_k=3):\n",
    "    \"\"\"\n",
    "    Evaluate RAG on a set of examples\n",
    "    \n",
    "    Returns:\n",
    "        predictions dict in HotpotQA format\n",
    "    \"\"\"\n",
    "    predictions = {\n",
    "        'answer': {},\n",
    "        'sp': {}  # supporting facts (we'll skip for now)\n",
    "    }\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(examples)\n",
    "    \n",
    "    for ex in tqdm(examples, desc=\"Evaluating\"):\n",
    "        try:\n",
    "            result = rag.answer(ex, top_k=top_k)\n",
    "            \n",
    "            predictions['answer'][ex['_id']] = result['answer']\n",
    "            predictions['sp'][ex['_id']] = []  # Empty for now\n",
    "            \n",
    "            # Simple exact match check\n",
    "            if result['answer'].lower().strip() == ex['answer'].lower().strip():\n",
    "                correct += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error on {ex['_id']}: {e}\")\n",
    "            predictions['answer'][ex['_id']] = \"\"\n",
    "            predictions['sp'][ex['_id']] = []\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n‚úÖ Simple Accuracy: {correct}/{total} ({100*accuracy:.1f}%)\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test on 20 examples first (adjust as needed)\n",
    "sample_size = 20\n",
    "print(f\"Testing on {sample_size} examples...\")\n",
    "print(\"‚ö†Ô∏è  This will make API calls. Cancel if you want to reduce sample size.\\n\")\n",
    "\n",
    "sample_data = dev_data[:sample_size]\n",
    "predictions = evaluate_rag(rag, sample_data, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions and Use Official Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "output_path = project_root / 'predictions_simple_rag.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "print(f\"‚úÖ Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use official evaluation (for the subset we tested)\n",
    "# We need to create a subset of dev data with only our tested examples\n",
    "\n",
    "tested_ids = set(predictions['answer'].keys())\n",
    "subset_dev = [ex for ex in dev_data if ex['_id'] in tested_ids]\n",
    "\n",
    "subset_path = project_root / 'dev_subset.json'\n",
    "with open(subset_path, 'w') as f:\n",
    "    json.dump(subset_dev, f)\n",
    "\n",
    "print(f\"Created subset with {len(subset_dev)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run official evaluation\n",
    "from evaluation.eval import eval\n",
    "\n",
    "print(\"Running official HotpotQA evaluation...\\n\")\n",
    "eval(str(output_path), str(subset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results: Where Does It Fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples where prediction was wrong\n",
    "wrong_examples = []\n",
    "\n",
    "for ex in sample_data:\n",
    "    pred = predictions['answer'].get(ex['_id'], '')\n",
    "    if pred.lower().strip() != ex['answer'].lower().strip():\n",
    "        wrong_examples.append({\n",
    "            'question': ex['question'],\n",
    "            'predicted': pred,\n",
    "            'truth': ex['answer'],\n",
    "            'type': ex['type'],\n",
    "            'supporting_facts': ex['supporting_facts']\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(wrong_examples)} wrong predictions\\n\")\n",
    "\n",
    "# Show first 3 failures\n",
    "for i, err in enumerate(wrong_examples[:3], 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Failure {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Question: {err['question']}\")\n",
    "    print(f\"Type: {err['type']}\")\n",
    "    print(f\"Predicted: {err['predicted']}\")\n",
    "    print(f\"Truth: {err['truth']}\")\n",
    "    print(f\"Supporting facts needed: {len(err['supporting_facts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps to Improve\n",
    "\n",
    "### Current Limitations:\n",
    "1. ‚ùå **Simple keyword matching** - doesn't understand semantics\n",
    "2. ‚ùå **No multi-hop reasoning** - retrieves once, doesn't chain\n",
    "3. ‚ùå **No re-ranking** - just uses simple word overlap\n",
    "4. ‚ùå **Fixed top_k** - doesn't adapt to question complexity\n",
    "\n",
    "### Improvements (in order of impact):\n",
    "\n",
    "1. **Better Retrieval** (Notebook 3)\n",
    "   - Use BM25 (classic IR)\n",
    "   - Or use embedding-based retrieval (sentence-transformers)\n",
    "   - Or hybrid (BM25 + embeddings)\n",
    "\n",
    "2. **Multi-hop Retrieval** (Notebook 4)\n",
    "   - Retrieve ‚Üí Read ‚Üí Retrieve again\n",
    "   - Build reasoning chains\n",
    "\n",
    "3. **Better Prompting** (Quick Win)\n",
    "   - Add reasoning instructions\n",
    "   - Use chain-of-thought\n",
    "   - Extract supporting facts\n",
    "\n",
    "4. **Re-ranking**\n",
    "   - Re-score retrieved paragraphs with cross-encoder\n",
    "\n",
    "### Current Performance Baseline:\n",
    "You should see around:\n",
    "- **EM**: 20-30% (exact match)\n",
    "- **F1**: 30-40% (token overlap)\n",
    "\n",
    "### Target Performance:\n",
    "- **Good RAG system**: 50-60% EM, 60-70% F1\n",
    "- **SOTA**: 70%+ EM, 80%+ F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
