{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100f557",
   "metadata": {},
   "outputs": [],
   "source": "import os, bz2, json, tarfile\nfrom pathlib import Path\n\ndef chunk_text(text, chunk_size=200, overlap=50):\n    \"\"\"Split text into overlapping chunks.\"\"\"\n    words = text.split()\n    chunks = []\n    start = 0\n    while start < len(words):\n        end = start + chunk_size\n        chunks.append(\" \".join(words[start:end]))\n        start = end - overlap\n    return chunks\n\n# Use the tar.bz2 file directly instead of expecting an extracted directory\nWIKI_PATH = Path(\"../data/raw/enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2\")\nOUT = \"wiki_chunks.jsonl\"\n\n# Limit for testing - set to None to process entire dump\n# For quick testing: 100 articles (~30-50 chunks)\n# For development: 1000 articles (~300-500 chunks) \n# For production: 10000+ articles or None (full dump)\nMAX_ARTICLES = 500  # Start with 500 for reasonable test\n\ndef wiki_json_generator(max_articles=None):\n    \"\"\"\n    Read Wikipedia articles directly from tar.bz2 archive.\n    Each .bz2 file inside contains JSON objects (one per line).\n    \"\"\"\n    print(f\"Opening Wikipedia dump: {WIKI_PATH}\")\n    article_count = 0\n    \n    with tarfile.open(WIKI_PATH, 'r:bz2') as tar:\n        members = tar.getmembers()\n        # Filter to only .bz2 files\n        bz2_members = [m for m in members if m.name.endswith('.bz2') and m.isfile()]\n        \n        print(f\"Found {len(bz2_members)} .bz2 files in archive\")\n        \n        for member in bz2_members:\n            if max_articles and article_count >= max_articles:\n                print(f\"Reached article limit: {max_articles}\")\n                break\n                \n            try:\n                # Extract the compressed file\n                f = tar.extractfile(member)\n                if f is None:\n                    continue\n                \n                # Decompress the bz2 content\n                decompressed = bz2.decompress(f.read())\n                \n                # Each line is a separate JSON object\n                for line in decompressed.decode('utf-8').strip().split('\\n'):\n                    if not line.strip():\n                        continue\n                    \n                    if max_articles and article_count >= max_articles:\n                        break\n                    \n                    try:\n                        article_count += 1\n                        yield json.loads(line)\n                    except json.JSONDecodeError:\n                        continue\n                        \n            except Exception as e:\n                print(f\"Error processing {member.name}: {e}\")\n                continue\n\nprint(\"Starting to process Wikipedia articles and create chunks...\")\nprint(f\"Article limit: {MAX_ARTICLES if MAX_ARTICLES else 'None (all articles)'}\")\nchunk_count = 0\n\nwith open(OUT, \"w\") as out:\n    for art in wiki_json_generator(max_articles=MAX_ARTICLES):\n        title = art.get(\"title\", \"\")\n        \n        # Text is stored as a list of sentences in the dump\n        text_list = art.get(\"text\", [])\n        if isinstance(text_list, list):\n            text = ' '.join(text_list)\n        else:\n            text = str(text_list)\n        \n        if not text or len(text.split()) < 50:\n            continue\n        \n        chunks = chunk_text(text, chunk_size=200, overlap=50)\n        for i, ch in enumerate(chunks):\n            out.write(json.dumps({\n                \"id\": f\"{title}_{i}\",\n                \"title\": title,\n                \"text\": ch\n            }) + \"\\n\")\n            chunk_count += 1\n            \n            # Print progress every 50 chunks for smaller datasets\n            if chunk_count % 50 == 0:\n                print(f\"Processed {chunk_count} chunks...\")\n\nprint(f\"\\n✅ Done! Created {chunk_count} chunks in {OUT}\")\n\n# Show sample\nif chunk_count > 0:\n    with open(OUT, \"r\") as f:\n        sample = json.loads(f.readline())\n        print(f\"\\nSample chunk:\")\n        print(f\"  Title: {sample['title']}\")\n        print(f\"  Text: {sample['text'][:150]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16ff8c",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom mistralai import Mistral\nimport json\nimport numpy as np\nfrom tqdm import tqdm\nimport time\n\n# Initialize Mistral client\nclient = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n\nembeddings = []\nmetadatas = []\n\n# Batch processing to handle API rate limits\nBATCH_SIZE = 100  # Process in batches to save intermediate results\nbatch_num = 0\n\nprint(\"Loading chunks from wiki_chunks.jsonl...\")\n\n# Count total chunks first\nwith open(\"wiki_chunks.jsonl\", \"r\") as f:\n    total_chunks = sum(1 for _ in f)\n\nprint(f\"Total chunks to embed: {total_chunks}\")\n\ntry:\n    with open(\"wiki_chunks.jsonl\", \"r\") as f:\n        for idx, line in enumerate(tqdm(f, desc=\"Embedding chunks\", total=total_chunks)):\n            obj = json.loads(line)\n            text = obj[\"text\"]\n\n            try:\n                # Call Mistral API (note: parameter is 'inputs' not 'input')\n                resp = client.embeddings.create(\n                    model=\"mistral-embed\",\n                    inputs=[text]\n                )\n\n                vector = resp.data[0].embedding  # 1024-dim vector\n\n                embeddings.append(np.array(vector, dtype=\"float32\"))\n                metadatas.append({\n                    \"id\": obj[\"id\"],\n                    \"title\": obj[\"title\"],\n                    \"text\": obj[\"text\"]\n                })\n                \n                # Save intermediate results every BATCH_SIZE chunks\n                if (idx + 1) % BATCH_SIZE == 0:\n                    batch_num += 1\n                    # Save backup\n                    np.save(f\"wiki_vectors_batch{batch_num}.npy\", np.vstack(embeddings))\n                    print(f\"\\n✅ Saved batch {batch_num} ({len(embeddings)} vectors)\")\n                    \n            except Exception as e:\n                print(f\"\\nError embedding chunk {idx}: {e}\")\n                # Wait a bit before continuing in case of rate limit\n                time.sleep(1)\n                continue\n\nexcept KeyboardInterrupt:\n    print(\"\\n⚠️ Processing interrupted by user\")\n\n# Save final results\nif embeddings:\n    print(f\"\\nSaving final results...\")\n    np.save(\"wiki_vectors.npy\", np.vstack(embeddings))\n    print(f\"✅ Saved {len(embeddings)} embeddings to wiki_vectors.npy\")\n\n    with open(\"wiki_meta.jsonl\", \"w\") as out:\n        for m in metadatas:\n            out.write(json.dumps(m) + \"\\n\")\n    print(f\"✅ Saved metadata to wiki_meta.jsonl\")\n    \n    print(f\"\\nFinal statistics:\")\n    print(f\"  - Total embeddings: {len(embeddings)}\")\n    print(f\"  - Embedding dimension: {embeddings[0].shape[0]}\")\n    print(f\"  - Total size: {np.vstack(embeddings).nbytes / (1024**2):.2f} MB\")\nelse:\n    print(\"⚠️ No embeddings were created\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb2a10",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\n# First check if faiss is installed, if not provide instructions\ntry:\n    import faiss\n    print(\"✅ FAISS is installed\")\nexcept ImportError:\n    print(\"❌ FAISS not found. Installing faiss-cpu...\")\n    import subprocess\n    subprocess.check_call([\"pip\", \"install\", \"faiss-cpu\", \"-q\"])\n    import faiss\n    print(\"✅ FAISS installed successfully\")\n\n# Load vectors\nprint(\"\\nLoading embeddings...\")\nvectors = np.load(\"wiki_vectors.npy\")\nprint(f\"✅ Loaded {vectors.shape[0]} vectors with {vectors.shape[1]} dimensions\")\n\nd = vectors.shape[1]   # Should be 1024 dims from mistral-embed\n\n# Create FAISS index for cosine similarity\nprint(\"\\nCreating FAISS index...\")\nindex = faiss.IndexFlatIP(d)  # Inner product for cosine similarity\n\n# Normalize vectors to use cosine similarity (cosine = normalized dot product)\nprint(\"Normalizing vectors...\")\nfaiss.normalize_L2(vectors)\n\n# Add vectors to index\nprint(\"Adding vectors to index...\")\nindex.add(vectors)\n\n# Save index to disk\nprint(\"Saving index...\")\nfaiss.write_index(index, \"wiki_faiss.index\")\n\nprint(f\"\\n✅ FAISS index created successfully!\")\nprint(f\"   - Index size: {index.ntotal:,} vectors\")\nprint(f\"   - Dimensions: {d}\")\nprint(f\"   - Index type: Flat (exact search)\")\nprint(f\"   - Saved to: wiki_faiss.index\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e3612",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom mistralai import Mistral\nimport faiss\nimport numpy as np\nimport json\n\n# Initialize Mistral client\nclient = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n\n# Load metadata and FAISS index\nprint(\"Loading FAISS index...\")\nindex = faiss.read_index(\"wiki_faiss.index\")\nprint(f\"✅ Loaded index with {index.ntotal} vectors\")\n\nprint(\"Loading metadata...\")\nwith open(\"wiki_meta.jsonl\") as f:\n    metas = [json.loads(l) for l in f]\nprint(f\"✅ Loaded {len(metas)} metadata entries\")\n\ndef dense_retrieval(query, k=5):\n    \"\"\"\n    Retrieve top-k most relevant chunks for a query using dense retrieval.\n    \n    Args:\n        query: Query string\n        k: Number of results to return\n        \n    Returns:\n        List of metadata dictionaries for top-k results\n    \"\"\"\n    # Create embedding for query (note: parameter is 'inputs' not 'input')\n    resp = client.embeddings.create(\n        model=\"mistral-embed\",\n        inputs=[query]\n    )\n    q_vec = np.array(resp.data[0].embedding, dtype=\"float32\").reshape(1, -1)\n    \n    # Normalize for cosine similarity\n    faiss.normalize_L2(q_vec)\n\n    # Search\n    scores, ids = index.search(q_vec, k)\n\n    results = []\n    for score, idx in zip(scores[0], ids[0]):\n        result = metas[idx].copy()\n        result['score'] = float(score)\n        results.append(result)\n\n    return results\n\n\n# Test retrieval with multiple queries\ntest_queries = [\n    \"Who was Barack Obama's vice president?\",\n    \"What is the capital of France?\",\n    \"Who invented the telephone?\"\n]\n\nfor query in test_queries:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"QUERY: {query}\")\n    print(\"=\"*80)\n    \n    results = dense_retrieval(query, k=3)\n    \n    for i, r in enumerate(results, 1):\n        print(f\"\\n{i}. TITLE: {r['title']}\")\n        print(f\"   SCORE: {r['score']:.4f}\")\n        print(f\"   TEXT: {r['text'][:200]}...\")\n        print(\"   ---\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}