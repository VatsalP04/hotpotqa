{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9100f557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process Wikipedia articles and create chunks...\n",
      "Article limit: 500\n",
      "Opening Wikipedia dump: ../data/raw/enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2\n",
      "Found 15517 .bz2 files in archive\n",
      "Processed 50 chunks...\n",
      "Processed 100 chunks...\n",
      "Processed 150 chunks...\n",
      "Reached article limit: 500\n",
      "\n",
      "✅ Done! Created 161 chunks in wiki_chunks.jsonl\n",
      "\n",
      "Sample chunk:\n",
      "  Title: Bafia people\n",
      "  Text: Bafia (beukpak) people inhabit the Mbam region in the centre province of Cameroon. Their origins are said to share many similarities with those of the...\n"
     ]
    }
   ],
   "source": [
    "import os, bz2, json, tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(\" \".join(words[start:end]))\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Use the tar.bz2 file directly instead of expecting an extracted directory\n",
    "WIKI_PATH = Path(\"../data/raw/enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2\")\n",
    "OUT = \"wiki_chunks.jsonl\"\n",
    "\n",
    "# Limit for testing - set to None to process entire dump\n",
    "# For quick testing: 100 articles (~30-50 chunks)\n",
    "# For development: 1000 articles (~300-500 chunks) \n",
    "# For production: 10000+ articles or None (full dump)\n",
    "MAX_ARTICLES = 500  # Start with 500 for reasonable test\n",
    "\n",
    "def wiki_json_generator(max_articles=None):\n",
    "    \"\"\"\n",
    "    Read Wikipedia articles directly from tar.bz2 archive.\n",
    "    Each .bz2 file inside contains JSON objects (one per line).\n",
    "    \"\"\"\n",
    "    print(f\"Opening Wikipedia dump: {WIKI_PATH}\")\n",
    "    article_count = 0\n",
    "    \n",
    "    with tarfile.open(WIKI_PATH, 'r:bz2') as tar:\n",
    "        members = tar.getmembers()\n",
    "        # Filter to only .bz2 files\n",
    "        bz2_members = [m for m in members if m.name.endswith('.bz2') and m.isfile()]\n",
    "        \n",
    "        print(f\"Found {len(bz2_members)} .bz2 files in archive\")\n",
    "        \n",
    "        for member in bz2_members:\n",
    "            if max_articles and article_count >= max_articles:\n",
    "                print(f\"Reached article limit: {max_articles}\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Extract the compressed file\n",
    "                f = tar.extractfile(member)\n",
    "                if f is None:\n",
    "                    continue\n",
    "                \n",
    "                # Decompress the bz2 content\n",
    "                decompressed = bz2.decompress(f.read())\n",
    "                \n",
    "                # Each line is a separate JSON object\n",
    "                for line in decompressed.decode('utf-8').strip().split('\\n'):\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    if max_articles and article_count >= max_articles:\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        article_count += 1\n",
    "                        yield json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {member.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "print(\"Starting to process Wikipedia articles and create chunks...\")\n",
    "print(f\"Article limit: {MAX_ARTICLES if MAX_ARTICLES else 'None (all articles)'}\")\n",
    "chunk_count = 0\n",
    "\n",
    "with open(OUT, \"w\") as out:\n",
    "    for art in wiki_json_generator(max_articles=MAX_ARTICLES):\n",
    "        title = art.get(\"title\", \"\")\n",
    "        \n",
    "        # Text is stored as a list of sentences in the dump\n",
    "        text_list = art.get(\"text\", [])\n",
    "        if isinstance(text_list, list):\n",
    "            text = ' '.join(text_list)\n",
    "        else:\n",
    "            text = str(text_list)\n",
    "        \n",
    "        if not text or len(text.split()) < 50:\n",
    "            continue\n",
    "        \n",
    "        chunks = chunk_text(text, chunk_size=200, overlap=50)\n",
    "        for i, ch in enumerate(chunks):\n",
    "            out.write(json.dumps({\n",
    "                \"id\": f\"{title}_{i}\",\n",
    "                \"title\": title,\n",
    "                \"text\": ch\n",
    "            }) + \"\\n\")\n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Print progress every 50 chunks for smaller datasets\n",
    "            if chunk_count % 50 == 0:\n",
    "                print(f\"Processed {chunk_count} chunks...\")\n",
    "\n",
    "print(f\"\\n✅ Done! Created {chunk_count} chunks in {OUT}\")\n",
    "\n",
    "# Show sample\n",
    "if chunk_count > 0:\n",
    "    with open(OUT, \"r\") as f:\n",
    "        sample = json.loads(f.readline())\n",
    "        print(f\"\\nSample chunk:\")\n",
    "        print(f\"  Title: {sample['title']}\")\n",
    "        print(f\"  Text: {sample['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c16ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from wiki_chunks.jsonl...\n",
      "Total chunks to embed: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  63%|██████▎   | 101/161 [00:23<00:12,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved batch 1 (100 vectors)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks: 100%|██████████| 161/161 [00:38<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final results...\n",
      "✅ Saved 161 embeddings to wiki_vectors.npy\n",
      "✅ Saved metadata to wiki_meta.jsonl\n",
      "\n",
      "Final statistics:\n",
      "  - Total embeddings: 161\n",
      "  - Embedding dimension: 1024\n",
      "  - Total size: 0.63 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize Mistral client\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "# Batch processing to handle API rate limits\n",
    "BATCH_SIZE = 100  # Process in batches to save intermediate results\n",
    "batch_num = 0\n",
    "\n",
    "print(\"Loading chunks from wiki_chunks.jsonl...\")\n",
    "\n",
    "# Count total chunks first\n",
    "with open(\"wiki_chunks.jsonl\", \"r\") as f:\n",
    "    total_chunks = sum(1 for _ in f)\n",
    "\n",
    "print(f\"Total chunks to embed: {total_chunks}\")\n",
    "\n",
    "try:\n",
    "    with open(\"wiki_chunks.jsonl\", \"r\") as f:\n",
    "        for idx, line in enumerate(tqdm(f, desc=\"Embedding chunks\", total=total_chunks)):\n",
    "            obj = json.loads(line)\n",
    "            text = obj[\"text\"]\n",
    "\n",
    "            try:\n",
    "                # Call Mistral API (note: parameter is 'inputs' not 'input')\n",
    "                resp = client.embeddings.create(\n",
    "                    model=\"mistral-embed\",\n",
    "                    inputs=[text]\n",
    "                )\n",
    "\n",
    "                vector = resp.data[0].embedding  # 1024-dim vector\n",
    "\n",
    "                embeddings.append(np.array(vector, dtype=\"float32\"))\n",
    "                metadatas.append({\n",
    "                    \"id\": obj[\"id\"],\n",
    "                    \"title\": obj[\"title\"],\n",
    "                    \"text\": obj[\"text\"]\n",
    "                })\n",
    "                \n",
    "                # Save intermediate results every BATCH_SIZE chunks\n",
    "                if (idx + 1) % BATCH_SIZE == 0:\n",
    "                    batch_num += 1\n",
    "                    # Save backup\n",
    "                    np.save(f\"wiki_vectors_batch{batch_num}.npy\", np.vstack(embeddings))\n",
    "                    print(f\"\\n✅ Saved batch {batch_num} ({len(embeddings)} vectors)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError embedding chunk {idx}: {e}\")\n",
    "                # Wait a bit before continuing in case of rate limit\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️ Processing interrupted by user\")\n",
    "\n",
    "# Save final results\n",
    "if embeddings:\n",
    "    print(f\"\\nSaving final results...\")\n",
    "    np.save(\"wiki_vectors.npy\", np.vstack(embeddings))\n",
    "    print(f\"✅ Saved {len(embeddings)} embeddings to wiki_vectors.npy\")\n",
    "\n",
    "    with open(\"wiki_meta.jsonl\", \"w\") as out:\n",
    "        for m in metadatas:\n",
    "            out.write(json.dumps(m) + \"\\n\")\n",
    "    print(f\"✅ Saved metadata to wiki_meta.jsonl\")\n",
    "    \n",
    "    print(f\"\\nFinal statistics:\")\n",
    "    print(f\"  - Total embeddings: {len(embeddings)}\")\n",
    "    print(f\"  - Embedding dimension: {embeddings[0].shape[0]}\")\n",
    "    print(f\"  - Total size: {np.vstack(embeddings).nbytes / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"⚠️ No embeddings were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dffb2a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ FAISS not found. Installing faiss-cpu...\n",
      "✅ FAISS installed successfully\n",
      "\n",
      "Loading embeddings...\n",
      "✅ Loaded 161 vectors with 1024 dimensions\n",
      "\n",
      "Creating FAISS index...\n",
      "Normalizing vectors...\n",
      "Adding vectors to index...\n",
      "Saving index...\n",
      "\n",
      "✅ FAISS index created successfully!\n",
      "   - Index size: 161 vectors\n",
      "   - Dimensions: 1024\n",
      "   - Index type: Flat (exact search)\n",
      "   - Saved to: wiki_faiss.index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# First check if faiss is installed, if not provide instructions\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"✅ FAISS is installed\")\n",
    "except ImportError:\n",
    "    print(\"❌ FAISS not found. Installing faiss-cpu...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"faiss-cpu\", \"-q\"])\n",
    "    import faiss\n",
    "    print(\"✅ FAISS installed successfully\")\n",
    "\n",
    "# Load vectors\n",
    "print(\"\\nLoading embeddings...\")\n",
    "vectors = np.load(\"wiki_vectors.npy\")\n",
    "print(f\"✅ Loaded {vectors.shape[0]} vectors with {vectors.shape[1]} dimensions\")\n",
    "\n",
    "d = vectors.shape[1]   # Should be 1024 dims from mistral-embed\n",
    "\n",
    "# Create FAISS index for cosine similarity\n",
    "print(\"\\nCreating FAISS index...\")\n",
    "index = faiss.IndexFlatIP(d)  # Inner product for cosine similarity\n",
    "\n",
    "# Normalize vectors to use cosine similarity (cosine = normalized dot product)\n",
    "print(\"Normalizing vectors...\")\n",
    "faiss.normalize_L2(vectors)\n",
    "\n",
    "# Add vectors to index\n",
    "print(\"Adding vectors to index...\")\n",
    "index.add(vectors)\n",
    "\n",
    "# Save index to disk\n",
    "print(\"Saving index...\")\n",
    "faiss.write_index(index, \"wiki_faiss.index\")\n",
    "\n",
    "print(f\"\\n✅ FAISS index created successfully!\")\n",
    "print(f\"   - Index size: {index.ntotal:,} vectors\")\n",
    "print(f\"   - Dimensions: {d}\")\n",
    "print(f\"   - Index type: Flat (exact search)\")\n",
    "print(f\"   - Saved to: wiki_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562e3612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index...\n",
      "✅ Loaded index with 161 vectors\n",
      "Loading metadata...\n",
      "✅ Loaded 161 metadata entries\n",
      "\n",
      "================================================================================\n",
      "QUERY: Who was Barack Obama's vice president?\n",
      "================================================================================\n",
      "\n",
      "1. TITLE: Al-Mustansir (Cairo)\n",
      "   SCORE: 0.6535\n",
      "   TEXT: Al-Mustansir (Arabic: المستنصر بالله الثاني‎ ‎ ) Abu al-Qasim Ahmad was a member of the Abbasid house who was imprisoned by his nephew the Caliph al-Musta'sim in Baghdad. Following the sack of Baghdad...\n",
      "   ---\n",
      "\n",
      "2. TITLE: George Halas Jr.\n",
      "   SCORE: 0.6524\n",
      "   TEXT: George Stanley Halas Jr. (September 4, 1925 – December 16, 1979), nicknamed \"Mugs,\" was one of four presidents in the history of the Chicago Bears franchise of the National Football League (NFL). He w...\n",
      "   ---\n",
      "\n",
      "3. TITLE: Rob Waiz\n",
      "   SCORE: 0.6499\n",
      "   TEXT: Robert L. Waiz, Jr (born January 30, 1963) is a politician of Jeffersonville, Indiana. He works in real estate and has been on the city council and served as mayor. Waiz, a Democrat, was first elected...\n",
      "   ---\n",
      "\n",
      "================================================================================\n",
      "QUERY: What is the capital of France?\n",
      "================================================================================\n",
      "\n",
      "1. TITLE: List of companies of the Central African Republic\n",
      "   SCORE: 0.7132\n",
      "   TEXT: The Central African Republic is a landlocked country in Central Africa. It is bordered by Chad to the north, Sudan to the northeast, South Sudan to the east, the Democratic Republic of the Congo and t...\n",
      "   ---\n",
      "\n",
      "2. TITLE: Jean Petit (theologian)\n",
      "   SCORE: 0.7056\n",
      "   TEXT: Jean Petit (Jehan Petit, John Parvus) (b. most likely at Brachy, Caux, in Normandy, and certainly in the Diocese of Rouen, c. 1360 − 15 July 1411) was a French theologian and professor in the Universi...\n",
      "   ---\n",
      "\n",
      "3. TITLE: Police (1985 film)\n",
      "   SCORE: 0.7050\n",
      "   TEXT: Police is a 1985 French romantic crime drama film directed by Maurice Pialat and starring Gérard Depardieu, Sophie Marceau, and Sandrine Bonnaire. Written by Catherine Breillat, the film is about a mo...\n",
      "   ---\n",
      "\n",
      "================================================================================\n",
      "QUERY: Who invented the telephone?\n",
      "================================================================================\n",
      "\n",
      "1. TITLE: Sabon\n",
      "   SCORE: 0.7057\n",
      "   TEXT: Sabon is an old-style serif typeface designed by the German-born typographer and designer Jan Tschichold (1902–1974) in the period 1964–1967. It was released jointly by the Linotype, Monotype, and Ste...\n",
      "   ---\n",
      "\n",
      "2. TITLE: Harold Bell\n",
      "   SCORE: 0.7050\n",
      "   TEXT: Harold Bell (October 5, 1919 – December 4, 2009) was an American marketer and merchandising executive who co-created Woodsy Owl, the iconic mascot of the United States Forest Service. Bell created Woo...\n",
      "   ---\n",
      "\n",
      "3. TITLE: The Nutcracker and the Mouse King\n",
      "   SCORE: 0.6944\n",
      "   TEXT: \"The Nutcracker and the Mouse King\" (German: \"Nussknacker und Mausekönig\" ) is a story written in 1816 by German author E. T. A. Hoffmann, in which young Marie Stahlbaum's favorite Christmas toy, the ...\n",
      "   ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize Mistral client\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "# Load metadata and FAISS index\n",
    "print(\"Loading FAISS index...\")\n",
    "index = faiss.read_index(\"wiki_faiss.index\")\n",
    "print(f\"✅ Loaded index with {index.ntotal} vectors\")\n",
    "\n",
    "print(\"Loading metadata...\")\n",
    "with open(\"wiki_meta.jsonl\") as f:\n",
    "    metas = [json.loads(l) for l in f]\n",
    "print(f\"✅ Loaded {len(metas)} metadata entries\")\n",
    "\n",
    "def dense_retrieval(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most relevant chunks for a query using dense retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query: Query string\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of metadata dictionaries for top-k results\n",
    "    \"\"\"\n",
    "    # Create embedding for query (note: parameter is 'inputs' not 'input')\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"mistral-embed\",\n",
    "        inputs=[query]\n",
    "    )\n",
    "    q_vec = np.array(resp.data[0].embedding, dtype=\"float32\").reshape(1, -1)\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(q_vec)\n",
    "\n",
    "    # Search\n",
    "    scores, ids = index.search(q_vec, k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], ids[0]):\n",
    "        result = metas[idx].copy()\n",
    "        result['score'] = float(score)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test retrieval with multiple queries\n",
    "test_queries = [\n",
    "    \"Who was Barack Obama's vice president?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who invented the telephone?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = dense_retrieval(query, k=3)\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. TITLE: {r['title']}\")\n",
    "        print(f\"   SCORE: {r['score']:.4f}\")\n",
    "        print(f\"   TEXT: {r['text'][:200]}...\")\n",
    "        print(\"   ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
