{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HotpotQA with Full Wikipedia Dump RAG\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading the full Wikipedia dump (enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2)\n",
    "2. Building a simple retrieval index using embeddings\n",
    "3. Answering questions using RAG with Mistral\n",
    "\n",
    "**Dataset**: HotpotQA with full Wikipedia as knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/vatsalpatel/hotpotqa\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tarfile\n",
    "import bz2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Wikipedia Dump\n",
    "\n",
    "The Wikipedia dump is in tar.bz2 format. Each article contains:\n",
    "- Title\n",
    "- Text/Abstract\n",
    "- Links to other articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dump path: /Users/vatsalpatel/hotpotqa/data/raw/enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2\n",
      "File exists: True\n",
      "File size: 1.45 GB\n"
     ]
    }
   ],
   "source": [
    "# Path to Wikipedia dump\n",
    "wiki_dump_path = project_root / 'data/raw/enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2'\n",
    "\n",
    "print(f\"Wikipedia dump path: {wiki_dump_path}\")\n",
    "print(f\"File exists: {wiki_dump_path.exists()}\")\n",
    "\n",
    "if wiki_dump_path.exists():\n",
    "    file_size_gb = wiki_dump_path.stat().st_size / (1024**3)\n",
    "    print(f\"File size: {file_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikipedia_dump(dump_path: Path, max_articles: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load Wikipedia articles from tar.bz2 dump.\n",
    "\n",
    "    The dump contains multiple .bz2 files, each with one JSON object per line.\n",
    "    Each JSON has: id, url, title, text (list of sentences), text_with_links, etc.\n",
    "\n",
    "    Args:\n",
    "        dump_path: Path to the tar.bz2 file\n",
    "        max_articles: Maximum number of articles to load (for testing)\n",
    "\n",
    "    Returns:\n",
    "        List of article dictionaries with 'title' and 'text' keys\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "\n",
    "    print(f\"Opening Wikipedia dump: {dump_path.name}\")\n",
    "\n",
    "    with tarfile.open(dump_path, 'r:bz2') as tar:\n",
    "        members = tar.getmembers()\n",
    "        print(f\"Total files in archive: {len(members)}\")\n",
    "\n",
    "        # Filter to only process .bz2 files (not directories)\n",
    "        bz2_members = [m for m in members if m.name.endswith('.bz2') and m.isfile()]\n",
    "        print(f\"BZ2 files found: {len(bz2_members)}\")\n",
    "\n",
    "        if max_articles:\n",
    "            # Estimate how many files to process based on max_articles\n",
    "            # Assuming ~100-500 articles per file\n",
    "            max_files = min(max(1, max_articles // 100), len(bz2_members))\n",
    "            bz2_members = bz2_members[:max_files]\n",
    "            print(f\"Processing first {len(bz2_members)} files to get ~{max_articles} articles...\")\n",
    "\n",
    "        for member in tqdm(bz2_members, desc=\"Processing files\"):\n",
    "            if max_articles and len(articles) >= max_articles:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Extract the compressed file\n",
    "                f = tar.extractfile(member)\n",
    "                if f is None:\n",
    "                    continue\n",
    "\n",
    "                # Decompress the bz2 content\n",
    "                decompressed = bz2.decompress(f.read())\n",
    "\n",
    "                # Each line is a separate JSON object\n",
    "                for line in decompressed.decode('utf-8').strip().split('\\n'):\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        article_data = json.loads(line)\n",
    "\n",
    "                        # Extract title and text\n",
    "                        title = article_data.get('title', '')\n",
    "\n",
    "                        # Text is stored as a list of sentences\n",
    "                        text_list = article_data.get('text', [])\n",
    "                        if isinstance(text_list, list):\n",
    "                            text = ' '.join(text_list)\n",
    "                        else:\n",
    "                            text = str(text_list)\n",
    "\n",
    "                        if title and text:\n",
    "                            articles.append({\n",
    "                                'id': article_data.get('id', ''),\n",
    "                                'title': title,\n",
    "                                'text': text,\n",
    "                                'url': article_data.get('url', '')\n",
    "                            })\n",
    "\n",
    "                        if max_articles and len(articles) >= max_articles:\n",
    "                            break\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        # Skip malformed JSON lines\n",
    "                        continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {member.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n‚úÖ Loaded {len(articles):,} Wikipedia articles\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Wikipedia dump: enwiki-20171001-pages-meta-current-withlinks-abstracts.tar.bz2\n",
      "Total files in archive: 15674\n",
      "BZ2 files found: 15517\n",
      "Processing first 10 files to get ~1000 articles...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515fef6c7aff4270b720d6d3e65fd224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded 1,000 Wikipedia articles\n",
      "\n",
      "================================================================================\n",
      "SAMPLE ARTICLE\n",
      "================================================================================\n",
      "Title: One Night Stand (1984 film)\n",
      "\n",
      "Text preview (first 500 chars):\n",
      "One Night Stand is a 1984 film directed by John Duigan....\n"
     ]
    }
   ],
   "source": [
    "# Load a sample of Wikipedia articles for testing\n",
    "# Set max_articles=None to load all articles (will take time!)\n",
    "wiki_articles = load_wikipedia_dump(wiki_dump_path, max_articles=1000)\n",
    "\n",
    "# Show first article\n",
    "if wiki_articles:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE ARTICLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Title: {wiki_articles[0]['title']}\")\n",
    "    print(f\"\\nText preview (first 500 chars):\")\n",
    "    print(wiki_articles[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Simple Retrieval Index\n",
    "\n",
    "We'll use sentence-transformers to create embeddings for retrieval.\n",
    "This is a simple dense retrieval approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentence-transformers if not already installed\n",
    "!pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vatsalpatel/hotpotqa/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "‚úÖ Model loaded: 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"‚úÖ Model loaded: {embedding_model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_document_index(articles: List[Dict], batch_size: int = 32) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build embeddings index for Wikipedia articles.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "        batch_size: Batch size for embedding generation\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, list of titles)\n",
    "    \"\"\"\n",
    "    print(f\"Building index for {len(articles):,} articles...\")\n",
    "    \n",
    "    # Prepare text for embedding: combine title and text\n",
    "    texts = [f\"{article['title']}. {article['text'][:500]}\" for article in articles]\n",
    "    titles = [article['title'] for article in articles]\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Index built: {embeddings.shape}\")\n",
    "    return embeddings, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index for 1,000 articles...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de293a2e6594d7c8da90da169438a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index built: (1000, 384)\n",
      "\n",
      "Index statistics:\n",
      "  - Number of documents: 1,000\n",
      "  - Embedding dimensions: 384\n",
      "  - Total size in memory: 1.46 MB\n"
     ]
    }
   ],
   "source": [
    "# Build the index\n",
    "wiki_embeddings, wiki_titles = build_document_index(wiki_articles)\n",
    "\n",
    "print(f\"\\nIndex statistics:\")\n",
    "print(f\"  - Number of documents: {len(wiki_articles):,}\")\n",
    "print(f\"  - Embedding dimensions: {wiki_embeddings.shape[1]}\")\n",
    "print(f\"  - Total size in memory: {wiki_embeddings.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, top_k: int = 5) -> List[Tuple[str, str, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve most relevant documents for a query.\n",
    "    \n",
    "    Args:\n",
    "        query: Query string\n",
    "        top_k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of (title, text, score) tuples\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = np.dot(wiki_embeddings, query_embedding.T).flatten()\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Return results\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append((\n",
    "            wiki_articles[idx]['title'],\n",
    "            wiki_articles[idx]['text'],\n",
    "            float(similarities[idx])\n",
    "        ))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the first president of the United States?\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED DOCUMENTS\n",
      "================================================================================\n",
      "\n",
      "1. William Everhart (score: 0.3182)\n",
      "   William Everhart (May 17, 1785 ‚Äì October 30, 1868) was an entrepreneur and wealthy businessman from Pennsylvania.  He was responsible for developing much of West Chester and stimulating its economic g...\n",
      "\n",
      "2. Isaac Newton Evans (score: 0.3090)\n",
      "   Isaac Evans (July 29, 1827 ‚Äì December 3, 1901) was a Republican member of the U.S. House of Representatives from Pennsylvania....\n",
      "\n",
      "3. Lyndon Hardy (score: 0.3008)\n",
      "   Lyndon Mauriece Hardy is an American physicist, fantasy author, and business owner....\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "test_query = \"Who was the first president of the United States?\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RETRIEVED DOCUMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "retrieved = retrieve_documents(test_query, top_k=3)\n",
    "\n",
    "for i, (title, text, score) in enumerate(retrieved, 1):\n",
    "    print(f\"\\n{i}. {title} (score: {score:.4f})\")\n",
    "    print(f\"   {text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load HotpotQA Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 7,405 HotpotQA questions\n",
      "\n",
      "================================================================================\n",
      "TEST QUESTION\n",
      "================================================================================\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Answer: yes\n",
      "Type: comparison\n"
     ]
    }
   ],
   "source": [
    "# Load HotpotQA dev data\n",
    "hotpotqa_path = project_root / 'data/raw/hotpot_dev_distractor_v1.json'\n",
    "\n",
    "with open(hotpotqa_path, 'r') as f:\n",
    "    hotpotqa_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(hotpotqa_data):,} HotpotQA questions\")\n",
    "\n",
    "# Pick a test question\n",
    "test_example = hotpotqa_data[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST QUESTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Question: {test_example['question']}\")\n",
    "print(f\"Answer: {test_example['answer']}\")\n",
    "print(f\"Type: {test_example['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implement Simple RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load Mistral API\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "mistral_client = Mistral(api_key=os.getenv('MISTRAL_API_KEY'))\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "print(f\"‚úÖ Mistral client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_with_rag(question: str, top_k: int = 5, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG: Retrieve + Generate.\n",
    "    \n",
    "    Args:\n",
    "        question: Question to answer\n",
    "        top_k: Number of documents to retrieve\n",
    "        verbose: Print retrieval details\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    if verbose:\n",
    "        print(\"üîç Retrieving relevant documents...\")\n",
    "    \n",
    "    retrieved_docs = retrieve_documents(question, top_k=top_k)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
    "        for i, (title, _, score) in enumerate(retrieved_docs, 1):\n",
    "            print(f\"  {i}. {title} (score: {score:.4f})\")\n",
    "    \n",
    "    # Step 2: Format context\n",
    "    context_parts = []\n",
    "    for i, (title, text, _) in enumerate(retrieved_docs, 1):\n",
    "        context_parts.append(f\"Document {i}: {title}\\n{text}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    prompt = f\"\"\"Answer the question based on the provided documents. Be concise and direct.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nü§ñ Generating answer...\")\n",
    "    \n",
    "    response = mistral_client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test RAG on HotpotQA Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAG PIPELINE - ANSWERING QUESTION\n",
      "================================================================================\n",
      "\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "\n",
      "üîç Retrieving relevant documents...\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  1. Kevin McDonald (footballer, born 1988) (score: 0.3916)\n",
      "  2. Britt Woodman (score: 0.3907)\n",
      "  3. Kenny Nolan (score: 0.3632)\n",
      "  4. Robert Fisher Tomes (score: 0.3523)\n",
      "  5. Sam Wilder (American football) (score: 0.3418)\n",
      "\n",
      "ü§ñ Generating answer...\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "\n",
      "ü§ñ Predicted Answer: No relevant information about **Scott Derrickson** or **Ed Wood** is provided in the given documents. Cannot determine their nationalities.\n",
      "‚úÖ Ground Truth: yes\n",
      "\n",
      "‚úó DIFFERENT\n"
     ]
    }
   ],
   "source": [
    "# Answer the test question\n",
    "print(\"=\"*80)\n",
    "print(\"RAG PIPELINE - ANSWERING QUESTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nQuestion: {test_example['question']}\\n\")\n",
    "\n",
    "predicted_answer = answer_question_with_rag(\n",
    "    test_example['question'],\n",
    "    top_k=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nü§ñ Predicted Answer: {predicted_answer}\")\n",
    "print(f\"‚úÖ Ground Truth: {test_example['answer']}\")\n",
    "\n",
    "# Simple match check\n",
    "is_correct = predicted_answer.lower().strip() == test_example['answer'].lower().strip()\n",
    "print(f\"\\n{'‚úì CORRECT' if is_correct else '‚úó DIFFERENT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test on Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION 1/3\n",
      "================================================================================\n",
      "Q: Which band was formed first, Lit or Adorable?\n",
      "\n",
      "ü§ñ Predicted: Neither **Lit** nor **Adorable** is mentioned in the provided documents, so the answer cannot be determined from the given context.\n",
      "‚úÖ Truth: Adorable\n",
      "‚úó DIFFERENT\n",
      "\n",
      "================================================================================\n",
      "QUESTION 2/3\n",
      "================================================================================\n",
      "Q: Cave-In-Rock, Illinois was a stronghold for serial killer/bandit brothers who operated in which century?\n",
      "\n",
      "ü§ñ Predicted: Cave-In-Rock, Illinois, was a stronghold for the **Harpe brothers**, serial killer/bandit brothers who operated in the **late 18th century** (1790s).\n",
      "‚úÖ Truth: who operated in Tennessee, Kentucky, Illinois, and Mississippi, in the late eighteenth century.\n",
      "‚úó DIFFERENT\n",
      "\n",
      "================================================================================\n",
      "QUESTION 3/3\n",
      "================================================================================\n",
      "Q: Whose works are more likely to be seen in an art gallery, Hovsep Pushman or Armen Chakmakian?\n",
      "\n",
      "ü§ñ Predicted: Neither **Hovsep Pushman** nor **Armen Chakmakian** are mentioned in the provided documents. Therefore, I cannot determine whose works are more likely to be seen in an art gallery based on the given context.\n",
      "‚úÖ Truth: Hovsep Pushman\n",
      "‚úó DIFFERENT\n",
      "\n",
      "================================================================================\n",
      "Exact matches: 0/3\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Test on 3 random questions\n",
    "test_questions = random.sample(hotpotqa_data, 3)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(test_questions, 1):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"QUESTION {i}/3\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Q: {example['question']}\")\n",
    "    \n",
    "    # Get answer (non-verbose)\n",
    "    predicted = answer_question_with_rag(example['question'], top_k=3, verbose=False)\n",
    "    \n",
    "    print(f\"\\nü§ñ Predicted: {predicted}\")\n",
    "    print(f\"‚úÖ Truth: {example['answer']}\")\n",
    "    \n",
    "    is_match = predicted.lower().strip() == example['answer'].lower().strip()\n",
    "    results.append(is_match)\n",
    "    print(f\"{'‚úì MATCH' if is_match else '‚úó DIFFERENT'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Exact matches: {sum(results)}/{len(results)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Index for Reuse (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index saved to /Users/vatsalpatel/hotpotqa/data/cache/wiki_index.pkl\n",
      "File size: 1.82 MB\n"
     ]
    }
   ],
   "source": [
    "# Save the index to avoid rebuilding\n",
    "cache_dir = project_root / 'data/cache'\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "index_path = cache_dir / 'wiki_index.pkl'\n",
    "\n",
    "with open(index_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'articles': wiki_articles,\n",
    "        'embeddings': wiki_embeddings,\n",
    "        'titles': wiki_titles\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Index saved to {index_path}\")\n",
    "print(f\"File size: {index_path.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the index later:\n",
    "# with open(index_path, 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "#     wiki_articles = data['articles']\n",
    "#     wiki_embeddings = data['embeddings']\n",
    "#     wiki_titles = data['titles']\n",
    "# print(f\"‚úÖ Index loaded: {len(wiki_articles):,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Built:\n",
    "1. ‚úÖ Loaded Wikipedia dump from tar.bz2 file\n",
    "2. ‚úÖ Built dense retrieval index using sentence-transformers\n",
    "3. ‚úÖ Implemented simple RAG pipeline (Retrieve + Generate)\n",
    "4. ‚úÖ Tested on HotpotQA questions\n",
    "\n",
    "### Performance Notes:\n",
    "- This is a **simple baseline** - single-hop retrieval\n",
    "- HotpotQA requires **multi-hop reasoning**\n",
    "- May need to retrieve multiple times for bridge questions\n",
    "\n",
    "### Improvements to Try:\n",
    "1. **Better retrieval**: Use hybrid search (BM25 + dense)\n",
    "2. **Multi-hop**: Iteratively retrieve based on previous context\n",
    "3. **Re-ranking**: Add a cross-encoder for better ranking\n",
    "4. **Chunking**: Split long articles into smaller chunks\n",
    "5. **Index all Wikipedia**: Load full dump instead of sample\n",
    "\n",
    "### Next Steps:\n",
    "- Evaluate on full dev set\n",
    "- Implement proper EM/F1 metrics\n",
    "- Try different retrieval strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
