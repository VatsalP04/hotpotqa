{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HotpotQA Quickstart - Understanding the Data\n",
    "\n",
    "This notebook will help you:\n",
    "1. Load the HotpotQA dataset\n",
    "2. Understand the data structure\n",
    "3. Explore examples\n",
    "4. Test Mistral API on a few examples\n",
    "\n",
    "**Goal**: Get familiar with the data before building RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data (Simple Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/vatsalpatel/hotpotqa\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: /Users/vatsalpatel/hotpotqa/data/raw/hotpot_dev_distractor_v1.json\n",
      "‚úÖ Loaded 7,405 examples\n"
     ]
    }
   ],
   "source": [
    "# Simple function to load data\n",
    "def load_data_simple(split='dev'):\n",
    "    \"\"\"Load HotpotQA data - simple version\"\"\"\n",
    "    if split == 'train':\n",
    "        file_path = project_root / 'data/raw/hotpot_train_v1.1.json'\n",
    "    else:\n",
    "        file_path = project_root / 'data/raw/hotpot_dev_distractor_v1.json'\n",
    "    \n",
    "    print(f\"Loading from: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(data):,} examples\")\n",
    "    return data\n",
    "\n",
    "# Load dev data (smaller, faster to work with)\n",
    "dev_data = load_data_simple('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore One Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "üìù ID: 5a8b57f25542995d1e6f1371\n",
      "\n",
      "‚ùì Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "\n",
      "‚úÖ Answer: yes\n",
      "\n",
      "üîç Type: comparison\n",
      "\n",
      "üìä Level: hard\n",
      "\n",
      "üìö Number of context paragraphs: 10\n",
      "\n",
      "üéØ Number of supporting facts: 2\n"
     ]
    }
   ],
   "source": [
    "# Look at the first example\n",
    "example = dev_data[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìù ID: {example['_id']}\")\n",
    "print(f\"\\n‚ùì Question: {example['question']}\")\n",
    "print(f\"\\n‚úÖ Answer: {example['answer']}\")\n",
    "print(f\"\\nüîç Type: {example['type']}\")\n",
    "print(f\"\\nüìä Level: {example['level']}\")\n",
    "print(f\"\\nüìö Number of context paragraphs: {len(example['context'])}\")\n",
    "print(f\"\\nüéØ Number of supporting facts: {len(example['supporting_facts'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTEXT PARAGRAPHS (First 3)\n",
      "================================================================================\n",
      "\n",
      "--- Paragraph 1 ---\n",
      "Title: Ed Wood (film)\n",
      "Number of sentences: 3\n",
      "First sentence: Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood....\n",
      "\n",
      "\n",
      "--- Paragraph 2 ---\n",
      "Title: Scott Derrickson\n",
      "Number of sentences: 3\n",
      "First sentence: Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer....\n",
      "\n",
      "\n",
      "--- Paragraph 3 ---\n",
      "Title: Woodson, Arkansas\n",
      "Number of sentences: 5\n",
      "First sentence: Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the context structure\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONTEXT PARAGRAPHS (First 3)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (title, sentences) in enumerate(example['context'][:3]):\n",
    "    print(f\"\\n--- Paragraph {i+1} ---\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Number of sentences: {len(sentences)}\")\n",
    "    print(f\"First sentence: {sentences[0][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUPPORTING FACTS (Gold Facts Needed to Answer)\n",
      "================================================================================\n",
      "\n",
      "üìå Scott Derrickson [sentence 0]:\n",
      "   Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\n",
      "\n",
      "üìå Ed Wood [sentence 0]:\n",
      "   Edward Davis Wood Jr. (October 10, 1924 ‚Äì December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n"
     ]
    }
   ],
   "source": [
    "# Look at supporting facts\n",
    "print(\"=\" * 80)\n",
    "print(\"SUPPORTING FACTS (Gold Facts Needed to Answer)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for title, sent_id in example['supporting_facts']:\n",
    "    # Find the actual sentence\n",
    "    for para_title, sentences in example['context']:\n",
    "        if para_title == title:\n",
    "            if sent_id < len(sentences):\n",
    "                sentence = sentences[sent_id]\n",
    "                print(f\"\\nüìå {title} [sentence {sent_id}]:\")\n",
    "                print(f\"   {sentence}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total examples: 7,405\n",
      "\n",
      "Question Types:\n",
      "  - comparison: 1,487 (20.1%)\n",
      "  - bridge: 5,918 (79.9%)\n",
      "\n",
      "Difficulty Levels:\n",
      "  - hard: 7,405 (100.0%)\n",
      "\n",
      "Answer Types:\n",
      "  - Yes/No answers: 458 (6.2%)\n",
      "  - Span answers: 6,947 (93.8%)\n"
     ]
    }
   ],
   "source": [
    "# Quick statistics\n",
    "from collections import Counter\n",
    "\n",
    "# Question types\n",
    "types = Counter([ex['type'] for ex in dev_data])\n",
    "levels = Counter([ex['level'] for ex in dev_data])\n",
    "\n",
    "# Answer types\n",
    "yes_no_count = sum(1 for ex in dev_data if ex['answer'].lower() in ['yes', 'no'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal examples: {len(dev_data):,}\")\n",
    "print(f\"\\nQuestion Types:\")\n",
    "for qtype, count in types.items():\n",
    "    pct = 100 * count / len(dev_data)\n",
    "    print(f\"  - {qtype}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDifficulty Levels:\")\n",
    "for level, count in levels.items():\n",
    "    pct = 100 * count / len(dev_data)\n",
    "    print(f\"  - {level}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAnswer Types:\")\n",
    "pct_yesno = 100 * yes_no_count / len(dev_data)\n",
    "print(f\"  - Yes/No answers: {yes_no_count:,} ({pct_yesno:.1f}%)\")\n",
    "print(f\"  - Span answers: {len(dev_data) - yes_no_count:,} ({100-pct_yesno:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE QUESTIONS (Random 5)\n",
      "================================================================================\n",
      "\n",
      "Q: Under Which Stanmore born prime minister was John Gorton a serving minister? \n",
      "A: Harold Holt\n",
      "Type: bridge, Level: hard\n",
      "\n",
      "Q: Which song by Last One Picked appeared in a 2004 American teen musical comedy film directed by Sara Sugarman?\n",
      "A: Na Na\n",
      "Type: bridge, Level: hard\n",
      "\n",
      "Q: What former city, now the fourth-largest Russian city, was the Belarusian State Technological University evacuated to in 1941?\n",
      "A: Sverdlovsk\n",
      "Type: bridge, Level: hard\n",
      "\n",
      "Q: Which super bowl that took place at the Miami Dolphins home stadium, featured the San Francisco 49ers defeating the San Diego Chargers?\n",
      "A: Super Bowl XXIX\n",
      "Type: bridge, Level: hard\n",
      "\n",
      "Q: Which mountain, Masherbrum or Khunyang Chhish, is a taller mountain?\n",
      "A: Khunyang Chhish\n",
      "Type: comparison, Level: hard\n"
     ]
    }
   ],
   "source": [
    "# Look at a few different examples\n",
    "import random\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE QUESTIONS (Random 5)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for ex in random.sample(dev_data, 5):\n",
    "    print(f\"\\nQ: {ex['question']}\")\n",
    "    print(f\"A: {ex['answer']}\")\n",
    "    print(f\"Type: {ex['type']}, Level: {ex['level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Mistral API on a Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral API key loaded\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "# Check if API key is loaded\n",
    "if os.getenv('MISTRAL_API_KEY'):\n",
    "    print(\"‚úÖ Mistral API key loaded\")\n",
    "else:\n",
    "    print(\"‚ùå No Mistral API key found. Set MISTRAL_API_KEY in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral client initialized with model: mistral-large-latest\n"
     ]
    }
   ],
   "source": [
    "# Initialize Mistral client\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=os.getenv('MISTRAL_API_KEY'))\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "print(f\"‚úÖ Mistral client initialized with model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to Mistral...\n",
      "\n",
      "================================================================================\n",
      "MISTRAL PREDICTION (with all context)\n",
      "================================================================================\n",
      "\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "\n",
      "ü§ñ Predicted: Yes.\n",
      "\n",
      "‚úÖ Ground Truth: yes\n",
      "\n",
      "üìä Match: NO ‚úó\n"
     ]
    }
   ],
   "source": [
    "# Test on one example - WITH ALL CONTEXT (no retrieval yet)\n",
    "test_example = dev_data[0]\n",
    "\n",
    "# Format all context paragraphs\n",
    "context_text = \"\\n\\n\".join([\n",
    "    f\"Title: {title}\\n{' '.join(sentences)}\"\n",
    "    for title, sentences in test_example['context']\n",
    "])\n",
    "\n",
    "# Create prompt\n",
    "prompt = f\"\"\"Answer the question based on the provided context. Give a concise answer.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {test_example['question']}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"Sending request to Mistral...\")\n",
    "\n",
    "response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "predicted_answer = response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISTRAL PREDICTION (with all context)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuestion: {test_example['question']}\")\n",
    "print(f\"\\nü§ñ Predicted: {predicted_answer}\")\n",
    "print(f\"\\n‚úÖ Ground Truth: {test_example['answer']}\")\n",
    "print(f\"\\nüìä Match: {'YES ‚úì' if predicted_answer.lower() == test_example['answer'].lower() else 'NO ‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test on Multiple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 5 random examples...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Example 1/5\n",
      "================================================================================\n",
      "Q: What Ruben Fleischer film did No One's Gonna Love You appear in?\n",
      "\n",
      "ü§ñ Predicted: No connection exists in the context. *\"No One's Gonna Love You\"* is not mentioned in relation to Ruben Fleischer's film *30 Minutes or Less*.\n",
      "‚úÖ Truth: Zombieland\n",
      "‚úó WRONG\n",
      "\n",
      "================================================================================\n",
      "Example 2/5\n",
      "================================================================================\n",
      "Q: The city Charles Prince Airport is approximately 16 km northwest of was called Salisbury until what year?\n",
      "\n",
      "ü§ñ Predicted: 1982\n",
      "‚úÖ Truth: 1982\n",
      "‚úì CORRECT\n",
      "\n",
      "================================================================================\n",
      "Example 3/5\n",
      "================================================================================\n",
      "Q: Which author dedicated a 1985 romance novel to the author who did in 2009 and wrote under the pen name Gwyneth Moore?\n",
      "\n",
      "ü§ñ Predicted: None of the authors mentioned in the context.\n",
      "‚úÖ Truth: Eva Ibbotson\n",
      "‚úó WRONG\n",
      "\n",
      "================================================================================\n",
      "Example 4/5\n",
      "================================================================================\n",
      "Q: The Lance Todd Trophy is presented at a stadium located in what country?\n",
      "\n",
      "ü§ñ Predicted: England.\n",
      "‚úÖ Truth: England\n",
      "‚úó WRONG\n",
      "\n",
      "================================================================================\n",
      "Example 5/5\n",
      "================================================================================\n",
      "Q: What is the name of this French former footballer whom football fans refer the football player Wilfred Bamnjo to as?\n",
      "\n",
      "ü§ñ Predicted: Claude Mak√©l√©l√©.\n",
      "‚úÖ Truth: Mak√©l√©l√©\n",
      "‚úó WRONG\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Accuracy: 1/5 (20%)\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è Note: This is just exact string matching. The official eval is more sophisticated.\n"
     ]
    }
   ],
   "source": [
    "# Test on 5 examples (be careful of API costs!)\n",
    "def test_mistral(example, use_all_context=True):\n",
    "    \"\"\"Test Mistral on one example\"\"\"\n",
    "    \n",
    "    # Format context\n",
    "    if use_all_context:\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"Title: {title}\\n{' '.join(sentences)}\"\n",
    "            for title, sentences in example['context']\n",
    "        ])\n",
    "    else:\n",
    "        # Use only first 3 paragraphs (faster/cheaper)\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"Title: {title}\\n{' '.join(sentences)}\"\n",
    "            for title, sentences in example['context'][:3]\n",
    "        ])\n",
    "    \n",
    "    prompt = f\"\"\"Answer the question based on the context. Be concise.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {example['question']}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Test on 5 random examples\n",
    "print(\"Testing on 5 random examples...\\n\")\n",
    "\n",
    "test_examples = random.sample(dev_data, 5)\n",
    "correct = 0\n",
    "\n",
    "for i, ex in enumerate(test_examples, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i}/5\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    predicted = test_mistral(ex, use_all_context=False)  # Use only 3 paragraphs\n",
    "    \n",
    "    print(f\"Q: {ex['question']}\")\n",
    "    print(f\"\\nü§ñ Predicted: {predicted}\")\n",
    "    print(f\"‚úÖ Truth: {ex['answer']}\")\n",
    "    \n",
    "    # Simple exact match check\n",
    "    if predicted.lower().strip() == ex['answer'].lower().strip():\n",
    "        print(\"‚úì CORRECT\")\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(\"‚úó WRONG\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(f\"Accuracy: {correct}/5 ({100*correct/5:.0f}%)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\n‚ö†Ô∏è Note: This is just exact string matching. The official eval is more sophisticated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Understand Multi-hop Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-HOP EXAMPLE (2 hops)\n",
      "================================================================================\n",
      "\n",
      "‚ùì Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "\n",
      "‚úÖ Answer: yes\n",
      "\n",
      "üîç Type: comparison\n",
      "\n",
      "\n",
      "üéØ SUPPORTING FACTS (Why it needs 2 hops):\n",
      "================================================================================\n",
      "\n",
      "Hop 1: From 'Scott Derrickson'\n",
      "   ‚Üí Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\n",
      "\n",
      "Hop 2: From 'Ed Wood'\n",
      "   ‚Üí Edward Davis Wood Jr. (October 10, 1924 ‚Äì December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
      "\n",
      "================================================================================\n",
      "üí° INSIGHT: The answer requires connecting facts from multiple paragraphs!\n",
      "   This is why simple retrieval might fail - you need multi-hop reasoning.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Find a good multi-hop example\n",
    "def find_multihop_example(data, min_hops=2):\n",
    "    \"\"\"Find an example with multiple reasoning hops\"\"\"\n",
    "    for ex in data:\n",
    "        # Count unique paragraph titles in supporting facts\n",
    "        unique_titles = set([title for title, _ in ex['supporting_facts']])\n",
    "        if len(unique_titles) >= min_hops:\n",
    "            return ex, len(unique_titles)\n",
    "    return None, 0\n",
    "\n",
    "multihop_ex, num_hops = find_multihop_example(dev_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"MULTI-HOP EXAMPLE ({num_hops} hops)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n‚ùì Question: {multihop_ex['question']}\")\n",
    "print(f\"\\n‚úÖ Answer: {multihop_ex['answer']}\")\n",
    "print(f\"\\nüîç Type: {multihop_ex['type']}\")\n",
    "\n",
    "print(f\"\\n\\nüéØ SUPPORTING FACTS (Why it needs {num_hops} hops):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show supporting facts from different paragraphs\n",
    "context_dict = {title: sentences for title, sentences in multihop_ex['context']}\n",
    "\n",
    "for hop_num, (title, sent_id) in enumerate(multihop_ex['supporting_facts'], 1):\n",
    "    if title in context_dict and sent_id < len(context_dict[title]):\n",
    "        sentence = context_dict[title][sent_id]\n",
    "        print(f\"\\nHop {hop_num}: From '{title}'\")\n",
    "        print(f\"   ‚Üí {sentence}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° INSIGHT: The answer requires connecting facts from multiple paragraphs!\")\n",
    "print(\"   This is why simple retrieval might fail - you need multi-hop reasoning.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You Learned:\n",
    "1. ‚úÖ HotpotQA data structure (questions, context, supporting facts)\n",
    "2. ‚úÖ Two types of questions: **bridge** and **comparison**\n",
    "3. ‚úÖ Multi-hop reasoning requires connecting facts from different paragraphs\n",
    "4. ‚úÖ Mistral can answer questions when given context\n",
    "\n",
    "### The Challenge for RAG:\n",
    "- Each example has **10 paragraphs** (2 gold + 8 distractors)\n",
    "- You need to **retrieve the right paragraphs** before generating\n",
    "- For multi-hop questions, you might need to **retrieve multiple times**\n",
    "\n",
    "### Next Notebook:\n",
    "- `02_simple_rag.ipynb` - Build a basic RAG system with retrieval\n",
    "\n",
    "### What You'll Need for RAG:\n",
    "1. **Retrieval**: Find relevant paragraphs (BM25, embeddings, or hybrid)\n",
    "2. **Re-ranking**: Improve retrieved results\n",
    "3. **Multi-hop**: Chain retrieval for complex questions\n",
    "4. **Generation**: Use Mistral to generate answers\n",
    "5. **Evaluation**: Measure EM, F1 scores on dev set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
